{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 当代人工智能实验一：文本分类\n",
    "## ——Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 一. 引入必要模块\n",
    "numpy将用于数据的处理。\n",
    "time用于记录代码运行时间。\n",
    "nltk用于实现分词与去停用词，gensim用于实现Word2Vec。\n",
    "LogisticRegression用于进行逻辑回归。\n",
    "TfidfVectorizer用于进行TF-IDF值的计算。\n",
    "train_test_split用于进行训练集与验证集的划分。\n",
    "classification_report用于衡量模型的训练表现。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 二. 下载停用词"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 三. 读取训练与测试数据\n",
    "每条训练集数据都包含“文本”与给定的“标签”。每条测试集数据都有编号与“文本”。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 打开并读取训练数据文档\n",
    "f_train = open('train_data.txt')\n",
    "train_text = f_train.read()\n",
    "# print(train_text)\n",
    "# 观察数据特征，确定数据文档中的每一项均由一个回车分割，故采取切片\n",
    "train_text = train_text.split(\"\\n\")\n",
    "# 创造两个数组，存储训练数据\n",
    "# labelList 存储每个数据的标签\n",
    "# rawList 存储每个数据的文本内容\n",
    "labelList = []\n",
    "rawList = []\n",
    "for i in range(len(train_text)-1):\n",
    "    train_text[i] = eval(train_text[i])\n",
    "    labelList.append(train_text[i][\"label\"])\n",
    "    rawList.append(train_text[i][\"raw\"])\n",
    "labelList = np.array(labelList)\n",
    "rawList = np.array(rawList)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 打开并读取测试数据文档\n",
    "f_test = open('test.txt')\n",
    "result = f_test.read()\n",
    "# print(test_text)\n",
    "# 观察数据特征，确定数据文档中的每一项均由一个回车分割，故采取切片\n",
    "result = result.split(\"\\n\")\n",
    "# 去除第一行\n",
    "result.pop(0)\n",
    "# 测试集的大小\n",
    "TEST_LENGTH = 2000\n",
    "result_id = list(range(TEST_LENGTH))\n",
    "result_text = []\n",
    "for i in range(TEST_LENGTH):\n",
    "    comma_index = result[i].find(\",\")\n",
    "    if comma_index != -1:\n",
    "        result_text.append(result[i][comma_index+2:])\n",
    "    else:\n",
    "        print(\"ERROR: COMMA NOT FOUND\")\n",
    "result_text = np.array(result_text)\n",
    "# result_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "ALL = np.append(rawList, result_text)\n",
    "# 记录数据的个数\n",
    "LENGTH_TRAIN = len(rawList)\n",
    "LENGTH_ALL = len(ALL)\n",
    "# ALL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 四. 使用Word2Vec进行LogisticRegression\n",
    "Word2Vec是将每个词进行向量化的过程。为了衡量一段文本的向量，我们将文本中的每个词对应的向量求和并求平均值，作为衡量段落向量的方法。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间:  15.154523611068726\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.19730163,  0.06354035,  0.0813565 , ..., -0.06578381,\n        -0.26233182, -0.02703605],\n       [-0.07018086,  0.02195625,  0.02862046, ..., -0.01766919,\n        -0.3069617 , -0.04676938],\n       [-0.18382921,  0.05678034,  0.03795623, ..., -0.04429037,\n        -0.24023913, -0.03515332],\n       ...,\n       [-0.2005331 ,  0.09426859, -0.03197032, ..., -0.02192535,\n        -0.2147369 ,  0.02126227],\n       [-0.19297416,  0.12304204,  0.02049277, ...,  0.23851865,\n        -0.17868695,  0.09136746],\n       [ 0.03058167,  0.07475319, -0.09276127, ...,  0.06028539,\n        -0.1637193 ,  0.01784925]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_texts: 包含了已经被分词的文本数据的列表。\n",
    "# vector_size: 指定每个单词的向量维度。\n",
    "# window: 模型在训练过程中，模型考虑了上下文的词语个数。\n",
    "# min_count: 只有出现次数不少于min_count次的单词才会被考虑。\n",
    "# sg: 这指代了我们使用了哪一种模型。sg=1表示使用了Skip-gram模型，即通过给定一个词来预测它的上下文。sg=0表示使用了CBOW模型，即通过给定上下文词来预测目标词。\n",
    "start_time = time.time()\n",
    "tokenized_texts = []\n",
    "for text in rawList:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            filtered_tokens.append(word)\n",
    "    tokenized_texts.append(filtered_tokens)\n",
    "\n",
    "model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, sg=1)\n",
    "text_vectors = []\n",
    "for tokenized_text in tokenized_texts:\n",
    "    text_vector = np.zeros(model.vector_size)\n",
    "    word_count = 0\n",
    "\n",
    "    for word in tokenized_text:\n",
    "        if word in model.wv:\n",
    "            text_vector = text_vector + model.wv[word]\n",
    "            word_count = word_count + 1\n",
    "\n",
    "    if word_count > 0:\n",
    "        text_vector = text_vector / word_count\n",
    "\n",
    "    text_vectors.append(text_vector)\n",
    "\n",
    "text_vectors = np.array(text_vectors)\n",
    "end_time = time.time()\n",
    "print(\"运行时间: \", end_time - start_time)\n",
    "text_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间： 5.794030666351318\n",
      "模型准确率： 0.7267999999999999\n",
      "模型精确度： 0.7308270142764218\n",
      "模型召回率： 0.7267999999999999\n",
      "模型F1-score： 0.7272711431073512\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集与测试集，这里选取12.5%的数据作为测试集，剩余数据作为训练集。\n",
    "# random_state数值是不同会让训练集与测试集不同，若写为None则每次都随机生成。\n",
    "start_time = time.time()\n",
    "accuracyTotal = 0\n",
    "precisionTotal = 0\n",
    "recallTotal = 0\n",
    "f1Total = 0\n",
    "LOOP_NUMBER = 5\n",
    "target_names = ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9']\n",
    "\n",
    "for loop in range(LOOP_NUMBER):\n",
    "    text_train, text_test, label_train, label_test = train_test_split(text_vectors, labelList, test_size=0.125, random_state=None)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(text_train, label_train)\n",
    "    accuracy = model.score(text_test, label_test)\n",
    "\n",
    "    y_pred = model.predict(text_test)\n",
    "    classification_rep = classification_report(label_test, y_pred, target_names=target_names, output_dict=True)\n",
    "\n",
    "    # 提取相应的指标值\n",
    "    precision = classification_rep['weighted avg']['precision']\n",
    "    recall = classification_rep['weighted avg']['recall']\n",
    "    f1 = classification_rep['weighted avg']['f1-score']\n",
    "\n",
    "    accuracyTotal += accuracy\n",
    "    precisionTotal += precision\n",
    "    recallTotal += recall\n",
    "    f1Total += f1\n",
    "\n",
    "# 计算平均值\n",
    "accuracy_avg = accuracyTotal / LOOP_NUMBER\n",
    "precision_avg = precisionTotal / LOOP_NUMBER\n",
    "recall_avg = recallTotal / LOOP_NUMBER\n",
    "f1_avg = f1Total / LOOP_NUMBER\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"运行时间：\", end_time - start_time)\n",
    "print(\"模型准确率：\", accuracy_avg)\n",
    "print(\"模型精确度：\", precision_avg)\n",
    "print(\"模型召回率：\", recall_avg)\n",
    "print(\"模型F1-score：\", f1_avg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们发现，准确率并不够理想。\n",
    "在这里，我们对Word2Vec的参数进行调整，调高vector_size增加向量的维数，调高window增加每个词考虑的上下文词语个数，增加min_count减少被考虑的词语个数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间:  28.94205117225647\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 0.07211074, -0.07428951, -0.09086505, ...,  0.01681455,\n        -0.07429978, -0.01893976],\n       [ 0.12664423, -0.03567159, -0.1087318 , ..., -0.0536377 ,\n        -0.06126191, -0.00337222],\n       [ 0.08269291, -0.0120166 , -0.10447612, ..., -0.02787858,\n        -0.08618506, -0.01647773],\n       ...,\n       [ 0.01953245, -0.06564941, -0.06536076, ...,  0.01694293,\n        -0.19201713,  0.03092945],\n       [ 0.0419427 , -0.10557784, -0.05688835, ...,  0.02594936,\n        -0.16388077, -0.00737843],\n       [ 0.06556548, -0.06687689, -0.09299784, ...,  0.00876947,\n        -0.13964166,  0.04956957]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_texts: 包含了已经被分词的文本数据的列表。\n",
    "# vector_size: 指定每个单词的向量维度。\n",
    "# window: 模型在训练过程中，模型考虑了上下文的词语个数。\n",
    "# min_count: 只有出现次数不少于min_count次的单词才会被考虑。\n",
    "# sg: 这指代了我们使用了哪一种模型。sg=1表示使用了Skip-gram模型，即通过给定一个词来预测它的上下文。sg=0表示使用了CBOW模型，即通过给定上下文词来预测目标词。\n",
    "start_time = time.time()\n",
    "tokenized_texts = []\n",
    "for text in rawList:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            filtered_tokens.append(word)\n",
    "    tokenized_texts.append(filtered_tokens)\n",
    "\n",
    "model = Word2Vec(tokenized_texts, vector_size=200, window=30, min_count=15, sg=1)\n",
    "text_vectors = []\n",
    "for tokenized_text in tokenized_texts:\n",
    "    text_vector = np.zeros(model.vector_size)\n",
    "    word_count = 0\n",
    "\n",
    "    for word in tokenized_text:\n",
    "        if word in model.wv:\n",
    "            text_vector = text_vector + model.wv[word]\n",
    "            word_count = word_count + 1\n",
    "\n",
    "    if word_count > 0:\n",
    "        text_vector = text_vector / word_count\n",
    "\n",
    "    text_vectors.append(text_vector)\n",
    "\n",
    "text_vectors = np.array(text_vectors)\n",
    "end_time = time.time()\n",
    "print(\"运行时间: \", end_time - start_time)\n",
    "text_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间： 4.952762842178345\n",
      "模型准确率： 0.8942\n",
      "模型精确度： 0.894911399801741\n",
      "模型召回率： 0.8942\n",
      "模型F1-score： 0.8939536882361626\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集与测试集，这里选取12.5%的数据作为测试集，剩余数据作为训练集。\n",
    "# random_state数值是不同会让训练集与测试集不同，若写为None则每次都随机生成。\n",
    "start_time = time.time()\n",
    "accuracyTotal = 0\n",
    "precisionTotal = 0\n",
    "recallTotal = 0\n",
    "f1Total = 0\n",
    "LOOP_NUMBER = 5\n",
    "target_names = ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9']\n",
    "\n",
    "for loop in range(LOOP_NUMBER):\n",
    "    text_train, text_test, label_train, label_test = train_test_split(text_vectors, labelList, test_size=0.125, random_state=None)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(text_train, label_train)\n",
    "    accuracy = model.score(text_test, label_test)\n",
    "\n",
    "    y_pred = model.predict(text_test)\n",
    "    classification_rep = classification_report(label_test, y_pred, target_names=target_names, output_dict=True)\n",
    "\n",
    "    # 提取相应的指标值\n",
    "    precision = classification_rep['weighted avg']['precision']\n",
    "    recall = classification_rep['weighted avg']['recall']\n",
    "    f1 = classification_rep['weighted avg']['f1-score']\n",
    "\n",
    "    accuracyTotal += accuracy\n",
    "    precisionTotal += precision\n",
    "    recallTotal += recall\n",
    "    f1Total += f1\n",
    "\n",
    "# 计算平均值\n",
    "accuracy_avg = accuracyTotal / LOOP_NUMBER\n",
    "precision_avg = precisionTotal / LOOP_NUMBER\n",
    "recall_avg = recallTotal / LOOP_NUMBER\n",
    "f1_avg = f1Total / LOOP_NUMBER\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"运行时间：\", end_time - start_time)\n",
    "print(\"模型准确率：\", accuracy_avg)\n",
    "print(\"模型精确度：\", precision_avg)\n",
    "print(\"模型召回率：\", recall_avg)\n",
    "print(\"模型F1-score：\", f1_avg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到，准确率获得了15%左右的提升。这是非常巨大的提升。\n",
    "实际上，Word2Vec是一个针对词语向量化的技术，而为了衡量一段文本的向量将所有词语的向量求和并求平均值，实际上也忽略了每个词各自的重要程度，这在逻辑上并不够严密。一个比较常用的方式是将每个词语的向量值乘以其TF-IDF值，因此在这里我们将进行尝试。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间为:  40.57243084907532\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 0.29739653, -0.40799748, -0.59443968, ..., -0.03522741,\n        -0.11605332, -0.07182262],\n       [ 0.86891719, -0.44466566, -1.27038685, ...,  0.00270309,\n         0.24782205, -0.25306409],\n       [ 0.63510258,  0.22956835, -1.07194985, ..., -0.39280625,\n        -0.16968163, -0.21911345],\n       ...,\n       [ 0.19282071,  0.03819563,  0.58099265, ..., -0.53214407,\n        -0.9995433 , -0.19288503],\n       [ 0.10797803, -0.2047696 , -0.23695472, ..., -0.38171517,\n        -1.12015148,  0.05703247],\n       [-0.57062523,  0.62114971, -0.28332049, ...,  0.37524278,\n        -2.14118437,  0.14609616]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tokenized_texts = []\n",
    "for text in rawList:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            filtered_tokens.append(word)\n",
    "    tokenized_texts.append(filtered_tokens)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(rawList)\n",
    "\n",
    "model = Word2Vec(tokenized_texts, vector_size=200, window=30, min_count=15, sg=1)\n",
    "text_vectors = []\n",
    "\n",
    "for i, tokenized_text in enumerate(tokenized_texts):\n",
    "    text_vector = np.zeros(model.vector_size)\n",
    "\n",
    "    for word in tokenized_text:\n",
    "        if word in model.wv:\n",
    "            word_index = tfidf_vectorizer.vocabulary_.get(word, -1)\n",
    "            if word_index != -1:\n",
    "                # 第i个文本的第word_index的TF-IDF值\n",
    "                tfidf_value = tfidf_matrix[i, word_index]\n",
    "                text_vector = text_vector + model.wv[word] * tfidf_value\n",
    "\n",
    "    text_vectors.append(text_vector)\n",
    "\n",
    "text_vectors = np.array(text_vectors)\n",
    "end_time = time.time()\n",
    "print(\"运行时间为: \", end_time - start_time)\n",
    "text_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间为： 15.353400468826294\n",
      "模型准确率： 0.9096\n",
      "模型精确度： 0.9106241785822583\n",
      "模型召回率： 0.9096\n",
      "模型F1-score： 0.9096586453335164\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集与测试集，这里选取12.5%的数据作为测试集，剩余数据作为训练集。\n",
    "# random_state数值是不同会让训练集与测试集不同，若写为None则每次都随机生成。\n",
    "start_time = time.time()\n",
    "accuracyTotal = 0\n",
    "precisionTotal = 0\n",
    "recallTotal = 0\n",
    "f1Total = 0\n",
    "LOOP_NUMBER = 5\n",
    "target_names = ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9']\n",
    "\n",
    "for loop in range(LOOP_NUMBER):\n",
    "    text_train, text_test, label_train, label_test = train_test_split(text_vectors, labelList, test_size=0.125, random_state=None)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(text_train, label_train)\n",
    "    accuracy = model.score(text_test, label_test)\n",
    "\n",
    "    y_pred = model.predict(text_test)\n",
    "    classification_rep = classification_report(label_test, y_pred, target_names=target_names, output_dict=True)\n",
    "\n",
    "    # 提取相应的指标值\n",
    "    precision = classification_rep['weighted avg']['precision']\n",
    "    recall = classification_rep['weighted avg']['recall']\n",
    "    f1 = classification_rep['weighted avg']['f1-score']\n",
    "\n",
    "    accuracyTotal += accuracy\n",
    "    precisionTotal += precision\n",
    "    recallTotal += recall\n",
    "    f1Total += f1\n",
    "\n",
    "# 计算平均值\n",
    "accuracy_avg = accuracyTotal / LOOP_NUMBER\n",
    "precision_avg = precisionTotal / LOOP_NUMBER\n",
    "recall_avg = recallTotal / LOOP_NUMBER\n",
    "f1_avg = f1Total / LOOP_NUMBER\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"运行时间为：\", end_time - start_time)\n",
    "print(\"模型准确率：\", accuracy_avg)\n",
    "print(\"模型精确度：\", precision_avg)\n",
    "print(\"模型召回率：\", recall_avg)\n",
    "print(\"模型F1-score：\", f1_avg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到，准确率有了微弱的提升，说明加权乘以TF-IDF是一个可能有效的做法。然而，即使如此，其效果仍然不如直接使用TF-IDF。\n",
    "基于这样的背景，我们只能认为对该问题而言，Word2Vec的性能劣于TF-IDF，不是最适合该问题的模型。\n",
    "**在接下来的问题分析中，我们会放弃使用Word2Vec作为文本转换为向量的方式。**"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
